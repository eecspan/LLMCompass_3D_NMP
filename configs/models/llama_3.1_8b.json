{
    "name": "llama-3.1-8b-instruct",
    "model_type": "llama",
    "hidden_size": 4096,
    "num_attention_heads": 32,
    "num_hidden_layers": 32,
    "num_key_value_heads": 8,
    "intermediate_size": 11008,
    "vocab_size": 128256,
    "data_type": "fp16",
    "device_count": 4
  }